# eval_sampling.py
import sys
from pathlib import Path

# âœ… å…³é”®ï¼šæŠŠé¡¹ç›®æ ¹ç›®å½•åŠ å…¥ sys.pathï¼ˆå¿…é¡»åœ¨ import model/trainer/dataset ä¹‹å‰ï¼‰
PROJECT_ROOT = Path(__file__).resolve().parents[1]  # .../minimind
sys.path.insert(0, str(PROJECT_ROOT))

# ä¸‹é¢å† import ä½ çš„é¡¹ç›®æ¨¡å—
import os
import time
import argparse
from dataclasses import dataclass

import torch

from model.model_minimind import MiniMindConfig
from trainer.trainer_utils import init_model, setup_seed

@dataclass
class GenCfg:
    max_new_tokens: int = 256
    do_sample: bool = True
    temperature: float = 0.8
    top_p: float = 0.9
    top_k: int = 50
    repetition_penalty: float = 1.10
    no_repeat_ngram_size: int = 0  # 0 è¡¨ç¤ºå…³é—­ï¼›æƒ³å¼ºåŠ›é˜²å¤è¯»å¯è®¾ 3~6


def build_prompt(tokenizer, user_text: str, system_text: str = "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚") -> str:
    messages = []
    if system_text:
        messages.append({"role": "system", "content": system_text})
    messages.append({"role": "user", "content": user_text})
    # add_generation_prompt=True: åœ¨æœ«å°¾åŠ ä¸Š assistant å¼€å§‹æ ‡è®°ï¼Œè®©æ¨¡å‹ç»­å†™
    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)


@torch.inference_mode()
def generate_one(model, tokenizer, prompt: str, device: str, cfg: GenCfg):
    inputs = tokenizer(prompt, return_tensors="pt")
    input_ids = inputs["input_ids"].to(device)

    pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id

    # è®¡æ—¶æ›´å‡†ï¼ˆCUDA åŒæ­¥ï¼‰
    if device.startswith("cuda"):
        torch.cuda.synchronize()
    t0 = time.time()

    gen_kwargs = dict(
    input_ids=input_ids,
    max_new_tokens=cfg.max_new_tokens,
    do_sample=cfg.do_sample,
    repetition_penalty=cfg.repetition_penalty,
    no_repeat_ngram_size=cfg.no_repeat_ngram_size,
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=pad_id,
    use_cache=True,
    )

    # âœ… åªæœ‰ sampling æ‰éœ€è¦è¿™äº›
    if cfg.do_sample:
        gen_kwargs.update(dict(
            temperature=cfg.temperature,
            top_p=cfg.top_p,
            top_k=cfg.top_k,
        ))

    out = model.generate(**gen_kwargs)

    if device.startswith("cuda"):
        torch.cuda.synchronize()
    t1 = time.time()

    gen_ids = out[0, input_ids.shape[1]:]
    text = tokenizer.decode(gen_ids, skip_special_tokens=True)

    speed = gen_ids.numel() / max(t1 - t0, 1e-9)
    return text.strip(), speed


def preset_to_cfg(preset: str) -> GenCfg:
    preset = preset.lower().strip()
    if preset == "chat":
        return GenCfg(
            max_new_tokens=256,
            do_sample=True,
            temperature=0.8,
            top_p=0.9,
            top_k=50,
            repetition_penalty=1.10,
            no_repeat_ngram_size=0,
        )
    if preset == "strong_anti_repeat":
        return GenCfg(
            max_new_tokens=256,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            top_k=50,
            repetition_penalty=1.15,
            no_repeat_ngram_size=4,
        )
    if preset == "code":
        # ä»£ç ä»»åŠ¡ä¸€èˆ¬æ›´é€‚åˆâ€œæ›´ä½æ¸©åº¦ + æ›´é«˜top_p + è¾ƒè½»é‡å¤æƒ©ç½šâ€
        return GenCfg(
            max_new_tokens=256,
            do_sample=True,
            temperature=0.2,
            top_p=0.95,
            top_k=50,
            repetition_penalty=1.05,
            no_repeat_ngram_size=0,
        )
    raise ValueError(f"Unknown preset: {preset}. Use chat / strong_anti_repeat / code")


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--weight", type=str, default="full_sft", help="æƒé‡åï¼ˆå’Œä½  eval_llm.py ç”¨æ³•ä¸€è‡´ï¼‰")
    parser.add_argument("--device", type=str, default="cuda:0" if torch.cuda.is_available() else "cpu")
    parser.add_argument("--dtype", type=str, default="bfloat16", choices=["bfloat16", "float16", "float32"])
    parser.add_argument("--preset", type=str, default="chat", help="chat / strong_anti_repeat / code")
    parser.add_argument("--mode", type=str, default="auto", choices=["auto", "interactive", "single"])
    parser.add_argument("--query", type=str, default="ä½ æœ‰ä»€ä¹ˆç‰¹é•¿ï¼Ÿ")
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--system", type=str, default="ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚")
    parser.add_argument("--do_sample", type=int, default=None, choices=[0, 1], help="0=greedy, 1=sampling")

    # æ¨¡å‹ç»“æ„ï¼ˆæŒ‰ä½ å½“å‰ 25.83M çš„é»˜è®¤ï¼šhidden=512, layers=8ï¼‰
    parser.add_argument("--hidden_size", type=int, default=512)
    parser.add_argument("--num_hidden_layers", type=int, default=8)
    parser.add_argument("--use_moe", type=int, default=0, choices=[0, 1])

    # å…è®¸ CLI è¦†ç›–éƒ¨åˆ†ç”Ÿæˆå‚æ•°
    parser.add_argument("--max_new_tokens", type=int, default=None)
    parser.add_argument("--temperature", type=float, default=None)
    parser.add_argument("--top_p", type=float, default=None)
    parser.add_argument("--top_k", type=int, default=None)
    parser.add_argument("--repetition_penalty", type=float, default=None)
    parser.add_argument("--no_repeat_ngram_size", type=int, default=None)

    
    args = parser.parse_args()

    setup_seed(args.seed)

    lm_config = MiniMindConfig(
        hidden_size=args.hidden_size,
        num_hidden_layers=args.num_hidden_layers,
        use_moe=bool(args.use_moe),
    )

    model, tokenizer = init_model(lm_config, args.weight, device=args.device)
    model.eval()

    # dtypeï¼ˆæ¨ç†ï¼‰
    if args.device.startswith("cuda"):
        if args.dtype == "float16":
            model = model.half()
        elif args.dtype == "bfloat16":
            model = model.to(dtype=torch.bfloat16)
        # float32 å°±ä¸åŠ¨

    cfg = preset_to_cfg(args.preset)

# âœ… è¿™é‡Œæ‰å¯ä»¥ç”¨ args
    if args.do_sample is not None:
        cfg.do_sample = bool(args.do_sample)
    # CLI è¦†ç›–
    for k in ["max_new_tokens", "temperature", "top_p", "top_k", "repetition_penalty", "no_repeat_ngram_size"]:
        v = getattr(args, k)
        if v is not None:
            setattr(cfg, k, v)

    auto_tests = [
        "ä½ æœ‰ä»€ä¹ˆç‰¹é•¿ï¼Ÿ",
        "ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„ï¼Ÿ",
        "è¯·ç”¨Pythonå†™ä¸€ä¸ªè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‡½æ•°ï¼Œç›´æ¥ç»™å‡ºä»£ç ã€‚",
        "è§£é‡Šä¸€ä¸‹â€œå…‰åˆä½œç”¨â€çš„åŸºæœ¬è¿‡ç¨‹ã€‚",
        "å¦‚æœæ˜å¤©ä¸‹é›¨ï¼Œæˆ‘åº”è¯¥å¦‚ä½•å‡ºé—¨ï¼Ÿè¯·ç»™å‡ºç®€æ´çš„è¦ç‚¹ã€‚",
        "æ¯”è¾ƒä¸€ä¸‹çŒ«å’Œç‹—ä½œä¸ºå® ç‰©çš„ä¼˜ç¼ºç‚¹ï¼ˆç”¨æ¡ç›®åˆ—å‡ºï¼Œé¿å…é‡å¤ï¼‰ã€‚",
        "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Œç”¨é€šä¿—è¯­è¨€ã€‚",
        "æ¨èä¸€äº›ä¸­å›½çš„ç¾é£Ÿï¼Œå°½é‡å¤šæ ·åŒ–ï¼Œä¸è¦é‡å¤ã€‚",
    ]

    def run_one(q: str):
        prompt = build_prompt(tokenizer, q, system_text=args.system)
        ans, speed = generate_one(model, tokenizer, prompt, args.device, cfg)
        print(f"\nğŸ’¬: {q}\nğŸ¤–: {ans}\n[Speed]: {speed:.2f} tokens/s")

    if args.mode == "single":
        run_one(args.query)
        return

    if args.mode == "auto":
        for q in auto_tests:
            run_one(q)
        return

    # interactive
    while True:
        q = input("\nğŸ’¬(è¾“å…¥ 'exit' é€€å‡º): ").strip()
        if not q or q.lower() == "exit":
            break
        run_one(q)


if __name__ == "__main__":
    main()